{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 现代卷积神经网络\n",
    "## 深度卷积神经网络\n",
    "### 学习表征\n",
    "- 让机器自动从原始数据中发现和提取对解决任务（如分类）有用的特征（即表征）的过程。\n",
    "- 如何学习表征（层层递进）：\n",
    "    - 先学习到一些简单的边缘、角点、颜色等基础特征（局部特征）；\n",
    "    - 然后将底层的特征组合起来，学习到更复杂的纹理、图案、部件。\n",
    "    - 最后将中间层的特征进一步组合，形成完整的、高度抽象的对象或概念； \n",
    "### $\\mathrm{AlexNet}$（$8$层卷积）\n",
    "![卷积神经网络](../image/AlexNet.jpg)\n",
    "- 激活函数改用 $\\mathrm{ReLU}$"
   ],
   "id": "eb424e567f070797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T06:50:51.106497Z",
     "start_time": "2025-09-03T06:50:37.794260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        \"\"\"使用自适应池化层来处理不同尺寸的输入\"\"\"\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"测试网络\"\"\"\n",
    "X = torch.rand(1, 1, 224, 224)\n",
    "net = AlexNet(num_classes=10)\n",
    "\n",
    "print(\"网络结构:\")\n",
    "print(net)\n",
    "\n",
    "print(\"\\n逐层输出形状：\")\n",
    "print(\"Input shape:\\t\", X.shape)\n",
    "\n",
    "# 手动计算每一层的输出形状\n",
    "with torch.no_grad():\n",
    "    x = X.clone()\n",
    "    for name, layer in net.named_children():\n",
    "        if name == 'features':\n",
    "            for i, sub_layer in enumerate(layer):\n",
    "                x = sub_layer(x)\n",
    "                print(f'{name}[{i}] ({sub_layer.__class__.__name__}) output shape:\\t', x.shape)\n",
    "        elif name == 'adaptive_pool':\n",
    "            x = layer(x)\n",
    "            print(f'{name} output shape:\\t', x.shape)\n",
    "        elif name == 'classifier':\n",
    "            x = x.view(x.size(0), -1)  # 展平\n",
    "            print(f'Flattened shape:\\t', x.shape)\n",
    "            for i, sub_layer in enumerate(layer):\n",
    "                x = sub_layer(x)\n",
    "                print(f'{name}[{i}] ({sub_layer.__class__.__name__}) output shape:\\t', x.shape)"
   ],
   "id": "d034c3cf4775452",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络结构:\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "逐层输出形状：\n",
      "Input shape:\t torch.Size([1, 1, 224, 224])\n",
      "features[0] (Conv2d) output shape:\t torch.Size([1, 96, 54, 54])\n",
      "features[1] (ReLU) output shape:\t torch.Size([1, 96, 54, 54])\n",
      "features[2] (MaxPool2d) output shape:\t torch.Size([1, 96, 26, 26])\n",
      "features[3] (Conv2d) output shape:\t torch.Size([1, 256, 26, 26])\n",
      "features[4] (ReLU) output shape:\t torch.Size([1, 256, 26, 26])\n",
      "features[5] (MaxPool2d) output shape:\t torch.Size([1, 256, 12, 12])\n",
      "features[6] (Conv2d) output shape:\t torch.Size([1, 384, 12, 12])\n",
      "features[7] (ReLU) output shape:\t torch.Size([1, 384, 12, 12])\n",
      "features[8] (Conv2d) output shape:\t torch.Size([1, 384, 12, 12])\n",
      "features[9] (ReLU) output shape:\t torch.Size([1, 384, 12, 12])\n",
      "features[10] (Conv2d) output shape:\t torch.Size([1, 256, 12, 12])\n",
      "features[11] (ReLU) output shape:\t torch.Size([1, 256, 12, 12])\n",
      "features[12] (MaxPool2d) output shape:\t torch.Size([1, 256, 5, 5])\n",
      "adaptive_pool output shape:\t torch.Size([1, 256, 6, 6])\n",
      "Flattened shape:\t torch.Size([1, 9216])\n",
      "classifier[0] (Dropout) output shape:\t torch.Size([1, 9216])\n",
      "classifier[1] (Linear) output shape:\t torch.Size([1, 4096])\n",
      "classifier[2] (ReLU) output shape:\t torch.Size([1, 4096])\n",
      "classifier[3] (Dropout) output shape:\t torch.Size([1, 4096])\n",
      "classifier[4] (Linear) output shape:\t torch.Size([1, 4096])\n",
      "classifier[5] (ReLU) output shape:\t torch.Size([1, 4096])\n",
      "classifier[6] (Linear) output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T10:55:17.926943Z",
     "start_time": "2025-08-27T10:55:17.806390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "resize = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(resize),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\"\"\"加载数据集\"\"\"\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "\"\"\"构建 DataLoader\"\"\"\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "6b8ca21f72b1d2f0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:27:30.194631Z",
     "start_time": "2025-08-27T11:04:55.854302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr, num_epochs = 0.01, 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = AlexNet(num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_hat = net(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(y_hat, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            \n",
    "    test_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_acc={test_acc:.4f}\")"
   ],
   "id": "9dca5da2719738ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.0180, train_acc=0.1076, test_acc=0.1379\n",
      "Epoch 2: train_loss=0.0179, train_acc=0.1088, test_acc=0.1001\n",
      "Epoch 3: train_loss=0.0106, train_acc=0.4617, test_acc=0.5778\n",
      "Epoch 4: train_loss=0.0075, train_acc=0.6217, test_acc=0.6464\n",
      "Epoch 5: train_loss=0.0063, train_acc=0.6897, test_acc=0.7263\n",
      "Epoch 6: train_loss=0.0056, train_acc=0.7305, test_acc=0.7569\n",
      "Epoch 7: train_loss=0.0051, train_acc=0.7575, test_acc=0.7686\n",
      "Epoch 8: train_loss=0.0046, train_acc=0.7772, test_acc=0.7828\n",
      "Epoch 9: train_loss=0.0043, train_acc=0.7940, test_acc=0.8053\n",
      "Epoch 10: train_loss=0.0040, train_acc=0.8098, test_acc=0.8082\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 使用块的网络（$\\mathrm{VGG}$）\n",
    "- 使用小尺寸卷积核代替大尺寸卷积核。\n",
    "### $\\mathrm{VGG}$块\n",
    "![VGG](../image/VGG.jpg)"
   ],
   "id": "eab1fc6390842db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T12:53:55.424489Z",
     "start_time": "2025-08-27T12:53:55.419199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "61404e08931f23a1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T13:00:08.672432Z",
     "start_time": "2025-08-27T13:00:08.654518Z"
    }
   },
   "cell_type": "code",
   "source": "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))",
   "id": "530c132d95a71ac7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T13:17:44.444110Z",
     "start_time": "2025-08-27T13:17:43.390165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vgg(conv_arch, in_channels=1, num_classes=10):\n",
    "    layers = []\n",
    "    out_channels = None\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        layers.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "        \n",
    "    conv_part = nn.Sequential(*layers)\n",
    "    \n",
    "    fc_part = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(4096, num_classes),\n",
    "    )\n",
    "    \n",
    "    net = nn.Sequential(conv_part, fc_part)\n",
    "    return net\n",
    "\n",
    "net = vgg(conv_arch, in_channels=1, num_classes=10)"
   ],
   "id": "b660b4c77961b3ed",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T13:18:47.308128Z",
     "start_time": "2025-08-27T13:18:47.165088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "\n",
    "for _, Blk in net.named_children():\n",
    "    for name, blk in Blk.named_children():\n",
    "        X = blk(X)\n",
    "        print(f\"{name} output shape:\\t\", X.shape)"
   ],
   "id": "5032bdd5c4dd9e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape:\t torch.Size([1, 64, 112, 112])\n",
      "1 output shape:\t torch.Size([1, 128, 56, 56])\n",
      "2 output shape:\t torch.Size([1, 256, 28, 28])\n",
      "3 output shape:\t torch.Size([1, 512, 14, 14])\n",
      "4 output shape:\t torch.Size([1, 512, 7, 7])\n",
      "0 output shape:\t torch.Size([1, 25088])\n",
      "1 output shape:\t torch.Size([1, 4096])\n",
      "2 output shape:\t torch.Size([1, 4096])\n",
      "3 output shape:\t torch.Size([1, 4096])\n",
      "4 output shape:\t torch.Size([1, 4096])\n",
      "5 output shape:\t torch.Size([1, 4096])\n",
      "6 output shape:\t torch.Size([1, 4096])\n",
      "7 output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T13:24:11.097717Z",
     "start_time": "2025-08-27T13:24:10.894565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ],
   "id": "908f20d49e2e1646",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T14:44:59.770940Z",
     "start_time": "2025-08-27T13:35:28.827418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_hat = net(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = y_hat.max(1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    train_loss = total_loss / total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            _, predicted = y_hat.max(1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        test_acc = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_acc={test_acc:.4f}\")"
   ],
   "id": "a2011029a6ff619b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.0180, train_acc=0.0996, test_acc=0.1000\n",
      "Epoch 2: train_loss=0.0180, train_acc=0.0969, test_acc=0.1000\n",
      "Epoch 3: train_loss=0.0180, train_acc=0.1011, test_acc=0.1000\n",
      "Epoch 4: train_loss=0.0177, train_acc=0.1845, test_acc=0.3522\n",
      "Epoch 5: train_loss=0.0069, train_acc=0.6548, test_acc=0.7322\n",
      "Epoch 6: train_loss=0.0038, train_acc=0.8178, test_acc=0.8519\n",
      "Epoch 7: train_loss=0.0029, train_acc=0.8631, test_acc=0.8683\n",
      "Epoch 8: train_loss=0.0025, train_acc=0.8846, test_acc=0.8552\n",
      "Epoch 9: train_loss=0.0022, train_acc=0.8949, test_acc=0.8829\n",
      "Epoch 10: train_loss=0.0020, train_acc=0.9045, test_acc=0.8870\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 网络中的网络（$\\mathrm{NiN}$）\n",
    "- 使用**小型的多层感知机**代替卷积核，实现非线性变换，增强表达能力。\n",
    "### $\\mathrm{NiN}$块\n",
    "- 卷积$+1\\times1$卷积组合的小模块\n",
    "\n",
    "![NiN块](../image/NiN.jpg)"
   ],
   "id": "517c43faa373808a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:24:54.121170Z",
     "start_time": "2025-08-28T14:24:54.116356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, stride, padding, use_relu=True):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                  stride=stride, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "    ]\n",
    "    if use_relu:\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)"
   ],
   "id": "375ccc3b80b11026",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### $\\mathrm{NiN}$模型",
   "id": "aac56f9da3a66302"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:17:51.372212Z",
     "start_time": "2025-08-28T14:17:51.354274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    # 先去掉 Dropout，等模型能收敛后再加\n",
    "    nin_block(384, 10, kernel_size=3, stride=1, padding=1, use_relu=False),  # 最后一层不要 ReLU\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    ")"
   ],
   "id": "ae7d9dfa5420c59a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:25:34.665802Z",
     "start_time": "2025-08-28T14:25:34.659529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr, num_epochs, batch_size = 0.01, 10, 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)"
   ],
   "id": "688b6cb324151527",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T14:50:31.581243Z",
     "start_time": "2025-08-28T14:26:59.668718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化，加速收敛\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "def train(net, train_iter, test_iter, num_epochs, device, optimizer, criterion):\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_loss, train_correct, total = 0, 0, 0\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * y.size(0)\n",
    "            train_correct += (y_hat.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        net.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_iter:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = net(X)\n",
    "                test_correct += (y_hat.argmax(1) == y).sum().item()\n",
    "                test_total += y.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"train loss {train_loss/total:.4f}, \"\n",
    "              f\"train acc {train_correct/total:.3f}, \"\n",
    "              f\"test acc {test_correct/test_total:.3f}\")\n",
    "\n",
    "train(net, train_iter, test_iter, num_epochs, device, optimizer, criterion)"
   ],
   "id": "fb74d97c695a0394",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 2.3030, train acc 0.099, test acc 0.172\n",
      "Epoch 2: train loss 2.3027, train acc 0.100, test acc 0.100\n",
      "Epoch 3: train loss 2.2459, train acc 0.138, test acc 0.267\n",
      "Epoch 4: train loss 1.2608, train acc 0.502, test acc 0.710\n",
      "Epoch 5: train loss 0.7279, train acc 0.733, test acc 0.736\n",
      "Epoch 6: train loss 0.6055, train acc 0.773, test acc 0.780\n",
      "Epoch 7: train loss 0.5368, train acc 0.801, test acc 0.817\n",
      "Epoch 8: train loss 0.4836, train acc 0.822, test acc 0.811\n",
      "Epoch 9: train loss 0.4461, train acc 0.836, test acc 0.843\n",
      "Epoch 10: train loss 0.4152, train acc 0.848, test acc 0.857\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 含并行连结的网络（$\\mathrm{GoogLeNet}$）\n",
    "- 使用并行的卷积层拼接 + 降维。\n",
    "### $\\mathrm{Inception}$ 模块\n",
    "- 多个并行的卷积分支\n",
    "\n",
    "![Inception块](../image/InceptionBlock.jpg)"
   ],
   "id": "50d1b34f69bc572d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:47:32.500461Z",
     "start_time": "2025-08-29T09:47:32.492461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4):\n",
    "        super(Inception, self).__init__()\n",
    "        # 线路1\n",
    "        self.p1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, c1, kernel_size=1),\n",
    "            nn.BatchNorm2d(c1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 线路2\n",
    "        self.p2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, c2[0], kernel_size=1),\n",
    "            nn.BatchNorm2d(c2[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(c2[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 线路3\n",
    "        self.p3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, c3[0], kernel_size=1),\n",
    "            nn.BatchNorm2d(c3[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(c3[1]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # 线路4\n",
    "        self.p4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, c4, kernel_size=1),\n",
    "            nn.BatchNorm2d(c4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.p1(x), self.p2(x), self.p3(x), self.p4(x)], dim=1)"
   ],
   "id": "525ee8f54a4c159",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## $\\mathrm{GoogLeNet}$模型\n",
    "\n",
    "![GoogLeNet](../image/GoogLeNetModel.jpg)"
   ],
   "id": "ef4ba3b6d55e655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:47:34.210710Z",
     "start_time": "2025-08-29T09:47:34.202711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MiniGoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MiniGoogLeNet, self).__init__()\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b3 = nn.Sequential(\n",
    "            Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "            Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b4 = nn.Sequential(\n",
    "            Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "            Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "            Inception(512, 128, (128, 256), (24, 64), 64),   \n",
    "            Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "            Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        \n",
    "        self.b5 = nn.Sequential(\n",
    "            Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "            Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.b5(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "a031f5c20c13e077",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T09:47:35.108410Z",
     "start_time": "2025-08-29T09:47:34.990202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(96),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "50892ab11f9b1f96",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T10:05:14.662861Z",
     "start_time": "2025-08-29T09:47:35.928041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------- 训练 ----------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = MiniGoogLeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    train_loss, train_correct, total = 0.0,0,0\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_correct += (y_hat.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    # 测试\n",
    "    net.eval()\n",
    "    test_correct, test_total = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            test_correct += (y_hat.argmax(1) == y).sum().item()\n",
    "            test_total += y.size(0)\n",
    "    net.train()\n",
    "    print(f\"Epoch {epoch+1}: train loss {train_loss/total:.4f}, \"\n",
    "          f\"train acc {train_correct/total:.3f}, \"\n",
    "          f\"test acc {test_correct/test_total:.3f}\")"
   ],
   "id": "142025fe6e23af6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.4132, train acc 0.848, test acc 0.846\n",
      "Epoch 2: train loss 0.2689, train acc 0.901, test acc 0.901\n",
      "Epoch 3: train loss 0.2293, train acc 0.917, test acc 0.907\n",
      "Epoch 4: train loss 0.2060, train acc 0.924, test acc 0.896\n",
      "Epoch 5: train loss 0.1859, train acc 0.932, test acc 0.905\n",
      "Epoch 6: train loss 0.1707, train acc 0.937, test acc 0.909\n",
      "Epoch 7: train loss 0.1550, train acc 0.943, test acc 0.927\n",
      "Epoch 8: train loss 0.1423, train acc 0.948, test acc 0.928\n",
      "Epoch 9: train loss 0.1307, train acc 0.952, test acc 0.910\n",
      "Epoch 10: train loss 0.1198, train acc 0.956, test acc 0.909\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 批量规范化\n",
    "### 训练深层网络\n",
    "- 批量规范化（BN）是在每一层输入上进行标准化（减均值、除方差），让特征分布保持稳定，从而加快收敛、缓解梯度消失/爆炸。它在此基础上再引入可学习的缩放和平移参数，保证网络仍能灵活表达特征。在$\\mathrm{CNN}$中，$\\mathrm{BN}$是对每个通道在整个 batch 和空间维度上做归一化。\n",
    "### 批量规范化层"
   ],
   "id": "c7a636ec15b485d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T16:56:31.837729Z",
     "start_time": "2025-08-29T16:56:31.830674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    \"\"\"判断是训练模式还是推理模式\"\"\"\n",
    "    if not X.requires_grad:  # 推理模式\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:  # 训练模式\n",
    "        assert X.dim() in (2, 4)\n",
    "        if X.dim() == 2:\n",
    "            \"\"\"全连接层 (batch, features)\"\"\"\n",
    "            mean = X.mean(dim=0, keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=0, keepdim=True)\n",
    "        else:\n",
    "            \"\"\"卷积层 (batch, channels, height, width)\"\"\"\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "\n",
    "        \"\"\"更新滑动平均（in-place 不要反向传播）\"\"\"\n",
    "        with torch.no_grad():\n",
    "            moving_mean[:] = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "            moving_var[:] = momentum * moving_var + (1.0 - momentum) * var\n",
    "\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, moving_mean, moving_var"
   ],
   "id": "57a2bddf32aed27a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:03:32.616083Z",
     "start_time": "2025-08-30T06:03:32.607083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims, eps=1e-12, momentum=0.9):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        if num_dims == 2:  # 全连接层\n",
    "            shape = (1, num_features)\n",
    "        else:  # 卷积层\n",
    "            shape = (1, num_features, 1, 1)\n",
    "\n",
    "        \"\"\"参与训练的参数 gamma 和 beta\"\"\"\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "\n",
    "        \"\"\"非模型参数（不参与梯度），滑动平均\"\"\"\n",
    "        self.register_buffer(\"moving_mean\", torch.zeros(shape))\n",
    "        self.register_buffer(\"moving_var\", torch.ones(shape))\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.training:  # 训练模式\n",
    "            if X.dim() == 2:  # 全连接层\n",
    "                mean = X.mean(dim=0, keepdim=True)\n",
    "                var = ((X - mean) ** 2).mean(dim=0, keepdim=True)\n",
    "            else:  # 卷积层 (N, C, H, W)\n",
    "                mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "                var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "            X_hat = (X - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "            \"\"\"更新滑动平均\"\"\"\n",
    "            with torch.no_grad():\n",
    "                self.moving_mean[:] = self.momentum * self.moving_mean + (1.0 - self.momentum) * mean\n",
    "                self.moving_var[:] = self.momentum * self.moving_var + (1.0 - self.momentum) * var\n",
    "\n",
    "        else:  # 推理模式\n",
    "            X_hat = (X - self.moving_mean) / torch.sqrt(self.moving_var + self.eps)\n",
    "\n",
    "        Y = self.gamma * X_hat + self.beta\n",
    "        return Y"
   ],
   "id": "cf720acb50be23bb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:03:33.036286Z",
     "start_time": "2025-08-30T06:03:33.026354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5),\n",
    "    BatchNorm(6, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    BatchNorm(16, num_dims=4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "    BatchNorm(256, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(256, 120),\n",
    "    BatchNorm(120, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(120, 84),\n",
    "    BatchNorm(84, num_dims=2),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(84, 10)\n",
    ")\n"
   ],
   "id": "1a5e0fe3486c0a64",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:05:25.431349Z",
     "start_time": "2025-08-30T06:03:33.842321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr, num_epochs, batch_size = 1.0, 10, 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, optimizer, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_loss, train_acc, total = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += l.item() * y.size(0)\n",
    "            train_acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        test_acc = evaluate_accuracy(net, test_iter, device)\n",
    "        print(f\"epoch {epoch+1}, loss {train_loss/total:.4f}, \"\n",
    "              f\"train acc {train_acc/total:.3f}, test acc {test_acc:.3f}\")\n",
    "\n",
    "def evaluate_accuracy(net, data_iter, device):\n",
    "    net.eval()\n",
    "    acc, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return acc / total\n",
    "\n",
    "train(net, train_iter, test_iter, loss, num_epochs, optimizer, device)"
   ],
   "id": "ad43ae3378f03949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7894, train acc 0.713, test acc 0.682\n",
      "epoch 2, loss 0.4780, train acc 0.825, test acc 0.779\n",
      "epoch 3, loss 0.4012, train acc 0.853, test acc 0.821\n",
      "epoch 4, loss 0.3566, train acc 0.870, test acc 0.828\n",
      "epoch 5, loss 0.3313, train acc 0.879, test acc 0.799\n",
      "epoch 6, loss 0.3088, train acc 0.887, test acc 0.786\n",
      "epoch 7, loss 0.2973, train acc 0.891, test acc 0.827\n",
      "epoch 8, loss 0.2802, train acc 0.897, test acc 0.831\n",
      "epoch 9, loss 0.2745, train acc 0.899, test acc 0.859\n",
      "epoch 10, loss 0.2644, train acc 0.903, test acc 0.833\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:31:32.611616Z",
     "start_time": "2025-08-30T06:31:32.601648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gamma = net[1].gamma.view(-1)\n",
    "beta = net[1].beta.view(-1)\n",
    "\n",
    "print(gamma, beta)"
   ],
   "id": "cb1a0ff857a4e701",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4113, 3.1100, 2.2536, 3.0580, 3.1238, 3.3374], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>) tensor([ 2.6976, -2.1039, -2.5220,  2.5038,  1.1402,  1.2099], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:46:58.879378Z",
     "start_time": "2025-08-30T06:46:58.866417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),  # 假设输入是单通道图像\n",
    "    nn.BatchNorm2d(6),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(16*4*4, 120),  # 输入特征维度要根据前面卷积+池化计算\n",
    "    nn.BatchNorm1d(120),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(120, 84),\n",
    "    nn.BatchNorm1d(84),\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ],
   "id": "89d7d7d13acb753f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T06:49:49.310970Z",
     "start_time": "2025-08-30T06:48:07.917614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr, num_epochs, batch_size = 1.0, 10, 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "def evaluate_accuracy(net, data_iter, device):\n",
    "    net.eval()\n",
    "    acc, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return acc / total\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, optimizer, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_loss, train_acc, total = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += l.item() * y.size(0)\n",
    "            train_acc += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        test_acc = evaluate_accuracy(net, test_iter, device)\n",
    "        print(f\"epoch {epoch+1}, loss {train_loss/total:.4f}, \"\n",
    "              f\"train acc {train_acc/total:.3f}, test acc {test_acc:.3f}\")\n",
    "\n",
    "train(net, train_iter, test_iter, loss, num_epochs, optimizer, device)"
   ],
   "id": "becd85c603cb2c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7853, train acc 0.715, test acc 0.601\n",
      "epoch 2, loss 0.4869, train acc 0.822, test acc 0.729\n",
      "epoch 3, loss 0.4149, train acc 0.848, test acc 0.806\n",
      "epoch 4, loss 0.3747, train acc 0.862, test acc 0.695\n",
      "epoch 5, loss 0.3535, train acc 0.869, test acc 0.847\n",
      "epoch 6, loss 0.3292, train acc 0.878, test acc 0.858\n",
      "epoch 7, loss 0.3129, train acc 0.884, test acc 0.797\n",
      "epoch 8, loss 0.3022, train acc 0.890, test acc 0.826\n",
      "epoch 9, loss 0.2889, train acc 0.894, test acc 0.844\n",
      "epoch 10, loss 0.2767, train acc 0.899, test acc 0.865\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 残差网络\n",
    "- 就是解决网络过深，训练损失反而上升的退化问题（使用残差块）。\n",
    "- **函数类**\n",
    "    - 每个残差块中要学习的映射（即残差）；\n",
    "### 残差块\n",
    "- 主路径（卷积层，批量归一化层、激活函数等组成）\n",
    "- 跳跃连接（将输入直接加到输出上面）\n",
    "\n",
    "![残差块](../image/ResidualBlock.jpg)"
   ],
   "id": "10b24dcd552e00e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T14:20:34.956116Z",
     "start_time": "2025-08-30T14:20:34.931115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"定义残差块（Residual Block）\"\"\"\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \"\"\"残差连接调整\"\"\"\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual  # 添加残差连接\n",
    "        out = F.relu(out)\n",
    "        return out"
   ],
   "id": "6d675cbda2b38b43",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### $\\mathrm{ResNet}$模型\n",
    "\n",
    "![残差模型](../image/ResNet.jpg)"
   ],
   "id": "19483637d4ea333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T14:34:46.097216Z",
     "start_time": "2025-08-30T14:34:46.089951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # 构建残差块\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "326de01fe7e4bcea",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T14:34:46.864679Z",
     "start_time": "2025-08-30T14:34:46.760518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"通过钩子打印每层的输出\"\"\"\n",
    "def print_shape_hook(module, output):\n",
    "    print(f\"Layer: {module.__class__.__name__}, Output shape: {output.shape}\")\n",
    "\n",
    "\"\"\"注册钩子函数\"\"\"\n",
    "def register_hooks(model):\n",
    "    hooks = []\n",
    "    for layer in model.children():\n",
    "        hook = layer.register_forward_hook(print_shape_hook)\n",
    "        hooks.append(hook)\n",
    "    return hooks\n",
    "\n",
    "model = SimpleResNet(num_classes=10)\n",
    "hooks = register_hooks(model)\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "\"\"\"清理钩子，避免内存泄漏\"\"\"\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ],
   "id": "a583897f97918386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d, Output shape: torch.Size([1, 64, 112, 112])\n",
      "Layer: BatchNorm2d, Output shape: torch.Size([1, 64, 112, 112])\n",
      "Layer: ReLU, Output shape: torch.Size([1, 64, 112, 112])\n",
      "Layer: MaxPool2d, Output shape: torch.Size([1, 64, 56, 56])\n",
      "Layer: Sequential, Output shape: torch.Size([1, 64, 56, 56])\n",
      "Layer: Sequential, Output shape: torch.Size([1, 128, 28, 28])\n",
      "Layer: Sequential, Output shape: torch.Size([1, 256, 14, 14])\n",
      "Layer: Sequential, Output shape: torch.Size([1, 512, 7, 7])\n",
      "Layer: AdaptiveAvgPool2d, Output shape: torch.Size([1, 512, 1, 1])\n",
      "Layer: Linear, Output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 稠密连接网络\n",
    "- 把前面所有层的输出拼接为稠密网络中某层的输入。\n",
    "### 从残差网络到稠密网络\n",
    "- 残差是层输出相加，稠密是层输出连接。\n",
    "### 稠密块体"
   ],
   "id": "63accd53e83e514b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:24:04.664215Z",
     "start_time": "2025-09-03T15:24:04.656934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
    "                               kernel_size=1, stride=1, bias=False)\n",
    "        \n",
    "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 复合函数：BN -> ReLU -> Conv\n",
    "        out = self.conv1(self.relu1(self.norm1(x)))\n",
    "        out = self.conv2(self.relu2(self.norm2(out)))\n",
    "        \n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        \n",
    "        # 将输入和输出在通道维度上拼接\n",
    "        out = torch.cat([x, out], 1)\n",
    "        return out"
   ],
   "id": "315cf41fe6eb3f8a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:24:24.349309Z",
     "start_time": "2025-09-03T15:24:24.344034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate,\n",
    "                bn_size,\n",
    "                drop_rate\n",
    "            )\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "id": "136b1dc59dc60a51",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 过渡层\n",
    "- 解决稠密块带来的通道数增加导致模型复杂化问题，使用$1\\times 1$卷积层来减少通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。"
   ],
   "id": "ced9def7ce3b9d59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:25:01.737221Z",
     "start_time": "2025-09-03T15:25:01.731323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class _Transition(nn.Module):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(num_input_features, num_output_features,\n",
    "                             kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.relu(self.norm(x)))\n",
    "        x = self.pool(x)\n",
    "        return x"
   ],
   "id": "b7f9a56ad9971f24",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:25:37.377487Z",
     "start_time": "2025-09-03T15:25:37.366981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        # 初始卷积层\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # 构建DenseBlock\n",
    "        num_features = num_init_features\n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        self.transitions = nn.ModuleList()\n",
    "        \n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.dense_blocks.append(block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(\n",
    "                    num_input_features=num_features,\n",
    "                    num_output_features=num_features // 2\n",
    "                )\n",
    "                self.transitions.append(trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # 最后的BN和ReLU\n",
    "        self.norm = nn.BatchNorm2d(num_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # 分类器\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "        \n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        \n",
    "        for i in range(len(self.dense_blocks)):\n",
    "            x = self.dense_blocks[i](x)\n",
    "            if i < len(self.transitions):\n",
    "                x = self.transitions[i](x)\n",
    "        \n",
    "        x = self.relu(self.norm(x))\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "id": "9b4c5363964bd28b",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:25:47.667923Z",
     "start_time": "2025-09-03T15:25:47.663741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建DenseNet-121模型\n",
    "def densenet121(num_classes=1000):\n",
    "    return DenseNet(\n",
    "        growth_rate=32,\n",
    "        block_config=(6, 12, 24, 16),\n",
    "        num_init_features=64,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "# 创建DenseNet-169模型\n",
    "def densenet169(num_classes=1000):\n",
    "    return DenseNet(\n",
    "        growth_rate=32,\n",
    "        block_config=(6, 12, 32, 32),\n",
    "        num_init_features=64,\n",
    "        num_classes=num_classes\n",
    "    )"
   ],
   "id": "c7b42f4b1952d8e0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:26:19.303593Z",
     "start_time": "2025-09-03T15:26:19.032141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 创建模型\n",
    "    model = densenet121(num_classes=10)\n",
    "    \n",
    "    # 测试输入\n",
    "    x = torch.randn(2, 3, 224, 224)\n",
    "    output = model(x)\n",
    "    print(f\"输出形状: {output.shape}\")"
   ],
   "id": "359fa5c4ca18aa48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 10])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T17:32:51.853809Z",
     "start_time": "2025-09-03T17:16:04.792418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\"数据预处理\"\"\"\n",
    "def get_data_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform\n",
    "\n",
    "\"\"\"加载数据集\"\"\"\n",
    "def load_datasets(train_transform, test_transform):\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\"\"\"训练函数\"\"\"\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计信息\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # 更新进度条\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\"\"\"验证函数\"\"\"\n",
    "def validate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "\"\"\"保存检查点\"\"\"\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, acc, path):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'accuracy': acc\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f'检查点已保存: {path}')\n",
    "\n",
    "\"\"\"加载检查点\"\"\"\n",
    "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        print(f'加载检查点: {checkpoint_path}')\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_acc = checkpoint['accuracy']\n",
    "        print(f'从 epoch {start_epoch} 继续训练')\n",
    "        return start_epoch, best_acc\n",
    "    return 0, 0\n",
    "\n",
    "def main():\n",
    "    \"\"\"超参数设置\"\"\"\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    num_epochs = 10\n",
    "    num_classes = 10\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用设备: {device}')\n",
    "    \n",
    "    model = densenet121(num_classes=num_classes)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_transform, test_transform = get_data_transforms()\n",
    "    train_dataset, test_dataset = load_datasets(train_transform, test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        momentum=momentum, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    \"\"\"学习率调度器\"\"\"\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[100, 150], gamma=0.1\n",
    "    )  # 在第5和8个epoch时调整学习率（这里为了方便就没使用到实际上，因为只训练十轮实际上），每次调整时学习率乘以0.1\n",
    "    \n",
    "    \"\"\"检查点设置\"\"\"\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'densenet121_best.pth')  # 这个可以帮助模型在训练中断时保存状态，以便于下次可以不用从头开始\n",
    "    \n",
    "    \"\"\"加载检查点（如果存在）\"\"\"\n",
    "    start_epoch, best_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "    \n",
    "    \"\"\"训练日志\"\"\"\n",
    "    log_file = open('training_log.txt', 'a')\n",
    "    log_file.write(f'开始训练 DenseNet-121\\n')\n",
    "    log_file.write(f'超参数: batch_size={batch_size}, lr={learning_rate}, epochs={num_epochs}\\n')\n",
    "    log_file.flush()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train_model(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        test_loss, test_acc = validate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "              f'Time: {epoch_time:.2f}s | '\n",
    "              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | '\n",
    "              f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | '\n",
    "              f'LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        \"\"\"保存日志\"\"\"\n",
    "        log_file.write(f'Epoch {epoch+1}: '\n",
    "                       f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                       f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%, '\n",
    "                       f'Time: {epoch_time:.2f}s\\n')\n",
    "        log_file.flush()\n",
    "        \n",
    "        \"\"\"保存最佳模型\"\"\"\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            save_checkpoint(model, optimizer, scheduler, epoch, best_acc, checkpoint_path)\n",
    "    \n",
    "    print('训练完成，进行最终测试...')\n",
    "    final_test_loss, final_test_acc = validate_model(model, test_loader, criterion, device)\n",
    "    print(f'最终测试结果: Loss: {final_test_loss:.4f}, Acc: {final_test_acc:.2f}%')\n",
    "    \n",
    "    log_file.write(f'最终测试结果: Loss: {final_test_loss:.4f}, Acc: {final_test_acc:.2f}%\\n')\n",
    "    log_file.close()\n",
    "    \n",
    "    # 保存最终模型\n",
    "    final_model_path = os.path.join(checkpoint_dir, 'densenet121_final.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f'最终模型已保存: {final_model_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "b19b31ef93f9df7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 391/391 [01:06<00:00,  5.90it/s, Loss=1.682, Acc=38.37%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Time: 90.52s | Train Loss: 1.6823 | Train Acc: 38.37% | Test Loss: 1.4262 | Test Acc: 47.62% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 391/391 [01:06<00:00,  5.84it/s, Loss=1.351, Acc=50.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] | Time: 91.41s | Train Loss: 1.3513 | Train Acc: 50.73% | Test Loss: 1.2571 | Test Acc: 54.86% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  49%|████▉     | 193/391 [00:44<00:21,  9.10it/s, Loss=1.209, Acc=56.40%]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000016E87A33740>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1664, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"D:\\Anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1622, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000016E87A33740>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1664, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"D:\\Anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1622, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "Epoch 3: 100%|██████████| 391/391 [01:08<00:00,  5.74it/s, Loss=1.189, Acc=57.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] | Time: 101.17s | Train Loss: 1.1893 | Train Acc: 57.17% | Test Loss: 1.1596 | Test Acc: 59.09% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 391/391 [01:13<00:00,  5.33it/s, Loss=1.072, Acc=61.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] | Time: 97.96s | Train Loss: 1.0717 | Train Acc: 61.57% | Test Loss: 1.0095 | Test Acc: 63.59% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 391/391 [01:07<00:00,  5.80it/s, Loss=0.978, Acc=65.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] | Time: 91.65s | Train Loss: 0.9778 | Train Acc: 65.19% | Test Loss: 1.0126 | Test Acc: 65.56% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 391/391 [01:07<00:00,  5.83it/s, Loss=0.906, Acc=67.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Time: 93.24s | Train Loss: 0.9055 | Train Acc: 67.82% | Test Loss: 0.9372 | Test Acc: 67.67% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 391/391 [01:17<00:00,  5.08it/s, Loss=0.849, Acc=69.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] | Time: 106.03s | Train Loss: 0.8488 | Train Acc: 69.65% | Test Loss: 0.8635 | Test Acc: 69.90% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 391/391 [01:18<00:00,  5.00it/s, Loss=0.799, Acc=71.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] | Time: 106.74s | Train Loss: 0.7993 | Train Acc: 71.45% | Test Loss: 0.8494 | Test Acc: 70.42% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 391/391 [01:14<00:00,  5.23it/s, Loss=0.752, Acc=73.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] | Time: 101.26s | Train Loss: 0.7517 | Train Acc: 73.50% | Test Loss: 0.8236 | Test Acc: 71.36% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 391/391 [01:11<00:00,  5.45it/s, Loss=0.717, Acc=74.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] | Time: 97.52s | Train Loss: 0.7169 | Train Acc: 74.81% | Test Loss: 0.7905 | Test Acc: 72.46% | LR: 0.010000\n",
      "检查点已保存: checkpoints\\densenet121_best.pth\n",
      "训练完成，进行最终测试...\n",
      "最终测试结果: Loss: 0.7905, Acc: 72.46%\n",
      "最终模型已保存: checkpoints\\densenet121_final.pth\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76a2830a7b9d9fde"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
